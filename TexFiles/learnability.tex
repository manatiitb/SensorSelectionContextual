%!TEX root =  main.tex
\vspace{-7pt}
Let $\TSA$ be the set of all stochastic, cascaded sensor acquisition problems. 
Thus, $\theta \in \TSA$ such that if $Y\sim \theta$ then $\gamma_k(\theta):=\Prob{Y\ne Y^k}$ 
is a decreasing sequence.
Given a subset $\Theta\subset \TSA$, we say that $\Theta$ is \emph{learnable} 
if there exists a learning algorithm $\Alg$ such that
for any $\theta\in \Theta$, the expected regret $\EE{ \Regret_n(\Alg,\theta) }$ 
of algorithm $\Alg$ on instance $\theta$ is sublinear.
A subset $\Theta$ is said to be a maximal learnable problem class if it is learnable and for any $\Theta'\subset \TSA$ superset
of $\Theta$, $\Theta'$ is not learnable.

\begin{defi}[Weak Dominance (WD)]
	An instance $\theta \in \TSA$  is said to satisfy the \emph{weak dominance,  property} if 
	for $i = a^*(\theta)$,
	\begin{align}
	\label{eq:wd} \rho = \min_{j > i} \frac{C_j - C_i}{\Prob{Y^i\ne Y^j}} \ge 1 %\forall j>i\,\,: \,\, C_j - C_i \ge \Prob{Y^i\ne Y^j}\,.
	\end{align}
We denote the set of all instances in $\TSA$ that satisfies this condition by $\TWD$.	
\end{defi}
\begin{thm}
The set $\TWD$ is essentially a maximal learnable set.
\end{thm}
