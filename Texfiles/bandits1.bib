%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Csaba at 2016-08-09 19:02:02 -0600 


%% Saved with string encoding Unicode (UTF-8) 

@book{costello,
author={S. Lin and D. J. Costello},
title={Error Control Coding: Fundamentals and Applications.},
publisher={Prentice Hall.}, 
year={1983}
}

@misc{voyager,
author={R. Ludwig and J. Taylor}, 
title={Voyager Telecommunications Manual, JPL DESCANSO (Design and Performance Summary Series)},
month={March} ,
year={2002.}
}


@article{baghdadian,
title={Effect of an Institutional
Triaging Algorithm on the Use
of Multidetector CT for Patients
with Blunt Abdominopelvic
Trauma over an 8-year Period},
author={A. H. Baghdanian and
A. A. Baghdanian and
A. Armetta and
M. Krastev and
T. Dechert and
P. Burke and
C. A. LeBedis and
S. W. Anderson and
J. A. Soto},
journal={Radiology},
year={2016}
}

@inproceedings{WGySz:NIPS15,
	Abstract = {We consider a sequential learning problem with Gaussian payoffs and side observations: after selecting an action i, the learner
receives information about the payoff of every action j in the form of Gaussian observations whose mean is the same as the mean payoff, but the variance depends on the pair (i,j) (and may be infinite). The setup allows a more refined information transfer from one action to another than previous partial monitoring setups, including the recently introduced graph-structured feedback case. For the first time in the literature, we provide non-asymptotic problem-dependent lower bounds on the regret of any algorithm, which recover existing asymptotic problem-dependent lower bounds and finite-time minimax lower bounds available in the literature. We also provide algorithms that achieve the problem-dependent lower bound (up to some universal constant factor) or the minimax lower bounds (up to  logarithmic factors). },
	Author = {Wu, Y. and Gy{\"o}rgy, A. and Szepesv{\'a}ri, {Cs}.},
	Booktitle = {NIPS},
	Date = {2015-09},
	Date-Added = {2016-08-10 01:01:02 +0000},
	Date-Modified = {2016-08-10 01:02:00 +0000},
	Keywords = {online learning, partial information, learning with side-observations, minimax bounds, finite-sample bounds, asymptotic optimality, minimax optimality},
	Month = {September},
	Pages = {1360--1368},
	Pdf = {papers/NIPS15-SideObs.pdf},
	Title = {Online Learning with Gaussian Payoffs and Side Observations},
	Year = {2015}}

@article{AgTeAn89:pmon,
	Author = {Agrawal, Rajeev and Teneketzis, Demosthenis and Anantharam, Venkatachalam},
	Date-Added = {2016-05-18 21:37:14 +0000},
	Date-Modified = {2016-05-18 21:37:14 +0000},
	Journal = {IEEE Transaction on Automatic Control},
	Pages = {258--267},
	Title = {Asymptotically efficient adaptive allocation schemes for controlled i.i.d. processes: Finite parameter space},
	Volume = {34},
	Year = {1989}}

@inproceedings{AlCeGeMa13,
	Author = {Alon, N. and Cesa-Bianchi, N. and Gentile, C. and Mansour, Y.},
	Booktitle = {Advances in Neural Information Processing Systems (NIPS)},
	Date-Added = {2016-05-18 21:03:28 +0000},
	Date-Modified = {2016-05-18 21:03:28 +0000},
	Title = {From Bandits to Experts: A Tale of Domination and Independence},
	Year = {2013}}

@article{Tho33,
	Author = {Thompson, W.R.},
	Date-Added = {2016-05-18 20:30:12 +0000},
	Date-Modified = {2016-05-18 20:30:12 +0000},
	Journal = {Biometrika},
	Pages = {285--294},
	Title = {On the likelihood that one unknown probability exceeds another in view of the evidence of two samples},
	Volume = {25},
	Year = {1933}}

@article{BaFoPaRaSze14,
	Abstract = { In a partial monitoring game, the learner repeatedly chooses an action, the
environment responds with an outcome, and then the learner suffers a loss and
receives a feedback signal, both of which are fixed functions of the action and
the outcome. The goal of the learner is to minimize his regret, which is the
difference between his total cumulative loss and the total loss of the best
fixed action in hindsight.
In this paper we characterize the minimax regret of any
partial monitoring game with finitely many actions and
outcomes. It turns out that the minimax regret of any such game is either zero,
Theta~(T^{1/2}), Theta(T^{2/3}), or Theta(T).  We provide computationally efficient learning
algorithms that achieve the minimax regret within logarithmic factor for any game. In addition to the bounds on the minimax regret, if we assume that the outcomes are generated in an i.i.d. fashion, we prove individual upper bounds on the expected regret.},
	Author = {Bart{\'o}k, G. and Foster, D. and P{\'a}l, D. and Rakhlin, A. and {Sz}epesv{\'a}ri, {Cs}.},
	Date = {2014-05},
	Date-Added = {2016-05-18 20:23:07 +0000},
	Date-Modified = {2016-05-18 20:23:07 +0000},
	Journal = {Mathematics of Operations Research},
	Keywords = {partial information, online learning, adversarial setting, theory, stochastic partial monitoring},
	Pages = {967--997},
	Pdf = {papers/partial_monitoring-mor.pdf},
	Title = {Partial monitoring -- classification, regret bounds, and algorithms},
	Volume = {39},
	Year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.tcs.2012.10.008}}

@inproceedings{ADORE-99,
	Author = {Draper, B. and Bins, J. and Baek, K.},
	Booktitle = {International Conference on Vision Systems},
	Date-Added = {2016-05-18 17:27:14 +0000},
	Date-Modified = {2016-05-18 17:27:14 +0000},
	Pages = {522--537},
	Title = {ADORE: Adaptive Object Recognition},
	Year = 1999}

@inproceedings{isukapalli01efficient-ICJAI,
	Author = {Isukapalli, R. and Greiner, R.},
	Booktitle = {International Joint Conference on Artificial Intelligence},
	Date-Added = {2016-05-18 17:26:59 +0000},
	Date-Modified = {2016-05-18 17:26:59 +0000},
	Pages = {1381--1387},
	Title = {Efficient Interpretation Policies},
	Year = {2001}}

@inproceedings{LCunderBudget-ECML05,
	Author = {Kapoor, A. and Greiner, R.},
	Booktitle = {{ECML}},
	Date-Added = {2016-05-18 17:26:59 +0000},
	Date-Modified = {2016-05-18 17:26:59 +0000},
	Title = {Learning and Classifying under Hard Budgets},
	Year = 2005}

@article{ActiveClass-AIJ-s,
	Author = {Greiner, R. and Grove, A. and Roth, D.},
	Date-Added = {2016-05-18 15:49:50 +0000},
	Date-Modified = {2016-05-18 15:49:50 +0000},
	Journal = {Artificial Intelligence},
	Pages = {137--174},
	Title = {Learning Cost-Sensitive Active Classifiers},
	Volume = 139,
	Year = 2002}

@inproceedings{poczos2009,
	Abstract = {An anytime algorithm is capable of returning a response to the given task at essentially any time; typically the quality of the response improves as the time increases. Here, we consider the challenge of learning when we should terminate such algorithms on each of a sequence of iid tasks, to optimize the expected average reward per unit time. We provide a system for addressing this challenge, which combines the global optimizer Cross- Entropy method with local gradient ascent. This paper theoretically investigates how far the estimated gradient is from the true gradient, then empirically demonstrates that this system is effective by applying it to a toy problem, as well as on a real-world face detection task.},
	Author = {P{\'o}czos, B. and Abbasi-Yadkori, Y. and Szepesv{\'a}ri, {Cs}. and Greiner, R. and Sturtevant, N.},
	Booktitle = {ICML},
	Date-Added = {2016-05-18 15:42:01 +0000},
	Date-Modified = {2016-05-18 15:42:01 +0000},
	Keywords = {reinforcement learning, application, pondering, gradient algorithm, REINFORCE, Cross-Entropy search},
	Pages = {825--832},
	Pdf = {papers/time_is_money-ICML09.pdf},
	Title = {Learning When to Stop Thinking and Do Something!},
	Year = {2009}}

@article{MOR11_LinearlyParametrized_RusmevichientongTsitsiklis,
	Author = {Rusmevichientong, Paat and Tsitsiklis, John N},
	Journal = {Mathematics of Operations Research},
	Number = {2},
	Pages = {395--411},
	Publisher = {INFORMS},
	Title = {Linearly Parameterized Bandits},
	Volume = {35},
	Year = {2010}}

@article{IC2000_AppleTasting_HelmboldLittlestoneLong,
	Author = {Helmboat, D. P. and Littlestone, PN and Long, P.M.},
	Journal = {Journal of Information and Computation},
	Number = {2},
	Pages = {85--139},
	Publisher = {ELSEVIER},
	Title = {Apple Tasting},
	Volume = {161},
	Year = {2000}}

@article{AAM85_Asymptotically_LaiRobbins,
	Author = {Lai, Tze Leung and Robbins, Herbert},
	Journal = {Journal of Advances in applied mathematics},
	Number = {1},
	Pages = {4--22},
	Publisher = {Elsevier},
	Title = {Asymptotically Efficient Adaptive Allocation Rules},
	Volume = {6},
	Year = {1985}}

@inproceedings{NIPS10_ParametricBandits_FillipiCappe,
	Author = {Filippi, Sarah and Cappe, Olivier and Garivier, Aur{\'e}lien and Szepesv{\'a}ri, Csaba},
	Booktitle = {Proceeding of Advances in Neural Information Processing Systems},
	Pages = {586--594},
	Title = {Parametric bandits: The Generalized Linear Case},
	Year = {2010}}

@inproceedings{NIPS2011_ImprovedAlgorithms_AbbasiPalSzepes,
	Author = {Abbasi-Yadkori, Yasin and P{\'a}l, D{\'a}vid and Szepesv{\'a}ri, Csaba},
	Booktitle = {Proceeding of Advances in Neural Information Processing Systems (NIPS)},
	Pages = {2312--2320},
	Title = {Improved Algorithms for Linear Stochastic Bandits},
	Year = {2011}}

@inproceedings{NFDS2010_LearningMultiUser_GaiKrishnamachariJain,
	Author = {Gai, Yi and Krishnamachari, Bhaskar and Jain, Rahul},
	Booktitle = {Proceeding of Symposium on New Frontiers in Dynamic Spectrum, 2010},
	Organization = {IEEE},
	Pages = {1--9},
	Title = {Learning Multiuser Channel Allocations in Cognitive Radio Networks: A Combinatorial Multi-armed Bandit Formulation},
	Year = {2010}}

@inproceedings{AISTATS2011_ContextualBandits_ChuLiReyzinSchapire,
	Author = {Chu, Wei and Li, Lihong and Reyzin, Lev and Schapire, Robert E.},
	Booktitle = {Proceeding of International Conference on Artificial Intelligence and Statistics (AISTATS)},
	Pages = {208--214},
	Title = {Contextual Bandits with Linear Payoff Functions},
	Year = {2011}}

@book{Book_IntroductionLinear_BertsimasTsitsiklis,
	Author = {Bertsimas, Dimitris and Tsitsiklis, John N.},
	Optedition = {edition},
	Publisher = {Athena Scientific, Belmont, Massachusetts},
	Title = {Introduction to Linear Optimization},
	Year = {2008}}

@article{JMLR02_UsingConfidenceBounds_Auer,
	Author = {Auer, P.},
	Journal = {Journal of Machine Learning Research},
	Pages = {397--422},
	Title = {Using Confidence Bounds for Exploitation-Exploration Trade-offs},
	Volume = {3},
	Year = {2002}}

@article{ML02_FiniteTimeAnalysis_AuerBianchiFischer,
	Author = {Auer, P. and Nichol{\'o}-Cesa-Bianchi and Fischer, Paul},
	Journal = {Journal of Machine Learning},
	Pages = {235--256},
	Publisher = {Springer},
	Title = {Finite-Time Analysis of Multiarmed Bandit Problem trade-offs},
	Volume = {3},
	Year = {2002}}

@inproceedings{ICML15_CheapBandits_HanawalSaligramaValko,
	Author = {Hanawal, M.K. and Saligrama, V. and Valko, M. and Munos, R.},
	Booktitle = {Proceeding of International Conference on Machine Learning (ICML)(to appear)},
	Title = {Cheap Bandits},
	Year = {2015}}

@inproceedings{Sigmetrics15_StochasticBanditsWithSideObservations_BuccapatnamEriyilmazShroff,
	Author = {Buccapatnam, S. and Eryilmaz, A. and Shroff, N. B.},
	Booktitle = {Proceeding of Sigmetrics},
	Title = {Stochastic Bandits With Side Observation on Networks},
	Year = {2014}}

@book{Book_RecommenderSystem_ZankerFelferningFriedtich,
	Author = {Jannach, D. and Zanker, M. and Felfernig, A. and Friedrich, G.},
	Optedition = {edition},
	Publisher = {Cambridge University Press},
	Title = {Recommender Systems: An Introduction},
	Year = {2010}}

@inproceedings{COLT08_RegretBoundsForSleeping_KelinbergMizilSharma,
	Author = {Kleinberg, Robert D. and Niculescu-mizil, Alexandru and Sharma, Yogeshwer},
	Booktitle = {Proceedings of Conference on Learning Theory},
	Pages = {425--436},
	Title = {Regret bounds for sleeping experts and bandits},
	Year = {2008}}

@article{AMS1952_SomeAspectsOfSequenntial_Robbins,
	Author = {Robbins, Herbert},
	Journal = {Bulletin of the American Mathematics Society},
	Keywords = {bandits},
	Pages = {527--535},
	Title = {Some aspects of the sequential design of experiments},
	Volume = {58},
	Year = {1952}}

@inproceedings{ICML14_SpectralBandits_ValkoMunos,
	Author = {Valko, Michal and Munos, R\'{e}mi and Kveton, Branislav and Koc\'{a}k, Tom\'{a}\v{s}},
	Booktitle = {Proceeding of International Conference on Machine Learning (ICML)},
	Title = {Spectral Bandits for Smooth Graph Functions},
	Year = {2014}}

@inproceedings{COLT09_ForcedExplorationBased_YadkoriAntosSzepe,
	Author = {Abbasi-Yadkori, Y. and Antos, A. and Szepesv\'{a}ri, Cs.},
	Booktitle = {Proceeding COLT workshop on On-line Learning with Limited Feedback},
	Title = {Forced-exploration based algorithms for playing in stochastic linear bandits},
	Year = {2009}}

@article{Algorithmica2003_ReinforcementLearning_AbeBiermanLong,
	Author = {Abe, N. and Biermann, A. W. and Long, P. M.},
	Journal = {Algorithmica},
	Keywords = {bandits},
	Pages = {263--293},
	Title = {Reinforcement learning with immediate rewards and linear hypotheses},
	Volume = {37},
	Year = {2003}}

@inproceedings{UAI09_ExploringCompactReinforment_WalshSzitaLittman,
	Author = {Walsh, T. J. and Szita, I. and Diuk, C. and Littman, M. L.},
	Booktitle = {Proceeding of Uncertainty in Artificial Intelligence (UAI)},
	Title = {Exploring compact reinforcement-learning representations with linear regression},
	Year = {2009}}

@inproceedings{ICML99_AssociativeReinformentLearning_AbeLong,
	Author = {Abe, N. and Long, P. M.},
	Booktitle = {Proceeding of International Conference on Machine Learning (ICML)},
	Title = {Associative reinforcement learning using linear probabilistic concepts},
	Year = {1999}}

@inproceedings{COLT09_MinimacPoliciesForAdversarial_AudibertBubeck,
	Author = {Audibert, J.-Y. and Bubeck, S.},
	Booktitle = {Proceedings of the Annual Conference on Learning Theory (COLT)},
	Title = {Minimax policies for adversarial and stochastic bandits},
	Year = {2009}}

@article{AAP1995_SampleMeanBased_Agarwal,
	Author = {Agrawal, R.},
	Journal = {Advances in Applied Probability},
	Keywords = {bandits},
	Pages = {1054--1078},
	Title = {Sample mean based index policies with $O(log n)$ regret for the multi-armed bandit problem},
	Volume = {27},
	Year = {1995}}

@inproceedings{AISTATS12_BanditTheoryMeetsCS_CarpentierMunos,
	Author = {Carpentier, Alexandra and Munos, R\'{e}mi},
	Booktitle = {Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)},
	Title = {Bandit theory meets compressed sensing for high dimensional stochastic linear bandit},
	Year = {2012}}

@inproceedings{AISTATS12_Online-to-confidence-set_AbbasiPalSzepes,
	Author = {Abbasi-Yadkori, Y. and Pal, D. and Szepesv\'{a}ri, Cs.},
	Booktitle = {Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)},
	Title = {Online-to-confidence-set conversions and application to sparse stochastic bandits},
	Year = {2012}}

@book{Book_IPredictionLearningAndGames_BianchiLugosi,
	Author = {Cesa-Bianchi, Nicol\'{o} and Lugosi, G\'{a}bor},
	Optedition = {edition},
	Publisher = {Cambridge University Press, New York},
	Title = {Prediction, Learning, and Games},
	Year = {2006}}

@article{JMLR2010_RegretBoundsAndMinimax_AudibertBubeck,
	Author = {Audibert, J.-Y. and Bubeck, S.},
	Journal = {Journal of Machine Learning Research},
	Keywords = {bandits},
	Pages = {2635--2686},
	Title = {Regret bounds and minimax policies under partial monitoring},
	Volume = {11},
	Year = {2010}}

@article{Hannan1957_HannanConsistency_Hannan,
	Author = {Hannan, J.},
	Journal = {Contributions to the Theory of Games},
	Pages = {97--139},
	Title = {Approximation to Bayes risk in repeated plays},
	Volume = {3},
	Year = {1957}}

@article{TCS09_ExplorationExploitationTradeOff_AudibertMunosSzepes,
	Author = {Audibert, Jean-Yves and Munos, R\'{e}mi and Szepesv\'{a}ri, Csaba},
	Journal = {Theoretical Computer Science},
	Keywords = {bandits},
	Pages = {1876--1902},
	Title = {Exploration--exploitation tradeoff using variance estimates in multi-armed bandits},
	Volume = {410},
	Year = {2009}}

@inproceedings{COLT11_TheKL-UCBAlgorithm_GarivierCappe,
	Author = {Garivier, A. and Capp\'{e}, O.},
	Booktitle = {Proceedings of the Annual Conference on Learning Theory (COLT)},
	Title = {The KL-UCB Algorithm for Bounded Stochastic Bandits and Beyond},
	Year = {2011}}

@inproceedings{WWW10_Contextaulbandits_LiChuWei,
	Address = {NC, USA},
	Author = {Li, L. and Wei, C. and Langford, J. and Schapire, R. E.},
	Booktitle = {Proceeding of International Word Wide Web conference, WWW},
	Month = {April},
	Title = {A Contextual-Bandit Approach to Personalized News Article Recommendation},
	Year = {2010}}

@inproceedings{COLT08_StochasticLinearOptimization_DaniHayesKakad,
	Address = {Helsinki, Finland},
	Author = {Dani, V. and Hayes, T. P. and Kakade, S. M.},
	Booktitle = {Proceeding of Conference on Learning Theory, COLT},
	Month = {July},
	Title = {Stochastic Linear Optimization under Bandit Feedback},
	Year = {2008}}

@article{MOR1952_LinearlyParamterizedBandits_RusmeTsitsi,
	Author = {Rusmevichientong, P. and Tsitsiklis, J. N.},
	Journal = {INFORMS, Mathematics of Operations Research},
	Month = {May},
	Number = {2},
	Pages = {395--411},
	Title = {Linearly Parameterized Bandits},
	Volume = {35},
	Year = {2010}}

@article{SIAMR02_ANonStochasticMultiArmed_AuerBiachiFreud,
	Author = {Auer, P. and Cesa-Bianchi, N. and Robert, Y. Freund and Schapire, E.},
	Journal = {SIAM Journal on Computing},
	Title = {The Non-stochastic Multi-armed Bandit Problem},
	Volume = {32},
	Year = {2003}}

@inproceedings{COLT14_ResourcefulContextual_BandaniLangSlivk,
	Address = {Barcelona, Spain},
	Author = {Badanidiyuru, A. and Langford, J. and Slivkins, A.},
	Booktitle = {Proceeding of Conference on Learning Theory, COLT},
	Month = {July},
	Title = {Resourceful Contextual Bandits},
	Year = {2014}}

@inproceedings{ZBGGySz13:CostlyFeatures,
	Author = {Zolghadr, N. and Bart\'ok, G. and Greiner, R. and Gy\"orgy, A. and Szepesv\'ari, C.},
	Booktitle = {NIPS},
	Date-Modified = {2016-05-18 21:31:56 +0000},
	Pages = {1241--1249},
	Title = {Online learning with costly features and labels},
	Year = {2013}}

@inproceedings{COLT15_OnlineLearningWithFeedback_AlonBianchiDekel,
	Author = {Alon, N. and Cesa-Biancbi, N. and Dekel, O. and Koren, T.},
	Booktitle = {Proceeding of Conference on Learning Theory},
	Pages = {23-35},
	Title = {Online learning with feedback graphs:Beyond bandits},
	Year = {2015}}

@inproceedings{NIPS13_FromBanditsToExperts_AlonBianchiGentile,
	Author = {Alon, N. and Cesa-Biancbi, N. and Gentile, C. and Mansour, Y.},
	Booktitle = {Proceeding of Conference on Neural Information Processing Systems, NIPS},
	Pages = {1610-1618},
	Title = {From bandits to experts: A tale of domination and independence},
	Year = {2013}}

@inproceedings{FOCS13_BanditsWithKnapsacks_BadanidiyuruKleinberg,
	Author = {A, Badanidiyuru and Kleinberg, R. and Slivkins, A.},
	Booktitle = {Proceeding of Conference on Foundations of Computer Science, FOCS},
	Pages = {207-216},
	Title = {Bandits with knapsacks},
	Year = {2013}}

@inproceedings{UAI14_ResourceAllocationSemiBandits_LattimoreCrammerCzepes,
	Author = {Lattimore, T. and Crammer, K. and \'ar, C. Szepesv},
	Booktitle = {Proceeding of Conference on Uncertainty and Artificial Intelligence, UAI},
	Pages = {477-486},
	Title = {Optimal resource allocation with semi-bandit feedback},
	Year = {2014}}

@inproceedings{MaSh11,
	Author = {Mannor, S. and Shamir, O.},
	Booktitle = {NIPS},
	Date-Modified = {2016-05-18 21:05:06 +0000},
	Title = {From bandits to experts: On the value of side-observations},
	Year = {2011}}

@inproceedings{SBCA14:BanditsPaid,
	Author = {Seldin, Y. and Bartlett, P. and Crammer, K. and Abbasi-Yadkori, Y.},
	Booktitle = {Proceeding of International Conference on Machine Learning, ICML},
	Date-Modified = {2016-05-18 21:30:43 +0000},
	Pages = {208-287},
	Title = {Prediction with limited advice and multiarmed bandits with paid observations},
	Year = {2014}}

@inproceedings{AAAI12_KnaspsackBased_TranChapmanRogersJennings,
	Author = {Tran-Thanh, L. and Chapman, A.C. and Rogers, A. and Jennings, N.R},
	Booktitle = {Proceeding of AAAI Conference on Artificial Intelligence},
	Title = {Knapsack based optimal policies for budget-limited multi-armed bandits},
	Year = {2012}}

@inproceedings{AISTATS13_SupervisedSequentialLearning_TrapezSaligram,
	Author = {Trapeznikov, K. and Saligrama, V.},
	Booktitle = {AISTATS},
	Date-Modified = {2016-05-18 21:18:12 +0000},
	Pages = {235--242},
	Title = {Supervised sequential classification under budget constraints},
	Year = {2013}}

@inproceedings{AISTATS12_ClassifierCascade_ChenXuWeinberger,
	Author = {Chen, M. and Xu, Z. and Weinberger, K. Q. and C, O. and Kedem, D.},
	Booktitle = {Proceeding of International Conference on Artificial Intelligence and Statistics, AISTATS},
	Title = {Classifier cascade: Tradeoffbetween accuracy and feature evaluation cost},
	Year = {2012}}

@article{ML13_MultistageClassifier_TrapezSaligramaCastanon,
	Author = {Trapeznikov, K. and Saligrama, V. and Castanon, D. A.},
	Date-Modified = {2016-05-18 21:18:36 +0000},
	Journal = {Machine Learning},
	Pages = {1--24},
	Title = {Multi-Stage Classifier Design},
	Volume = {39},
	Year = {2014}}

@inproceedings{ICML13_CostSensitiveTreeClassification_XuKusnerChenWeinberger,
	Author = {Xu, Z. and Kusner, M. and Chen, M. and Weinberger, K. Q.},
	Booktitle = {Proceeding of International Conference on Machine Learning, ICML},
	Pages = {133-141},
	Title = {Cost-sensitive tree of classifiers},
	Year = {2013}}

@book{Book_InferenceLearning_MacKay,
	Author = {Mackay, David. J. C.},
	Date = {2003},
	Publisher = {Cambridge University Press},
	Title = {Information Theory, Inference, and Learning Algorithms}
	}
	
@inproceedings{NIPS2015_DirectedAcyclic_WangTrapezSaligram,
	Author = {J. Wang and K. Trapeznikov and V. Saligrama},
	Booktitle = {Proceeding of Conference on Neural Information Processing Systems, NIPS},
	Title = {Directed Acyclic Graph For Resource Constrained Prediction},
	Year = {2015}}	

@inproceedings{ICML2015_FeatureBudgeted_NanWangSaligrama,
	Author = {F. Nan and J. Wang and V. Saligrama},
	Booktitle = {Proceedings of the International Conference on Machine Learning, ICML},
	Title = {Feature-Budgeted Random Forest},
	Year = {2015}}	
